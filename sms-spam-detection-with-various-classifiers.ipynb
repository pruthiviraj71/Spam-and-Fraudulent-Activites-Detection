{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"33ffe59a-78f7-f5b5-4671-b5ca53026e24"},"source":"Goal of this notebook to test several classifiers on the data set with different features "},{"cell_type":"markdown","metadata":{"_cell_guid":"a611e17a-e5af-d0ae-d3bf-a02d699f6fcc"},"source":"And beforehand i want to thank Jose Portilla for his magnificent \"Python for Data Science and Machine Learning\" course on Udemy , which helped me to dive into ML =)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a9d79990-beea-6157-59fa-d59a44b92ba8"},"source":"### Let's begin"},{"cell_type":"markdown","metadata":{"_cell_guid":"79182371-b9d8-4e8f-eeb6-b38f3476a674"},"source":"First of all neccesary imports"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6094c6e-9777-c7be-bd57-fe431519ad20"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\n%matplotlib inline"},{"cell_type":"markdown","metadata":{"_cell_guid":"6689da0a-5795-85f6-2f13-4eb276bde97f"},"source":"Let's read the data from csv file"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"94a28c87-ff0a-8302-05bd-404df2873761"},"outputs":[],"source":"sms = pd.read_csv('../input/spam.csv', encoding='latin-1')\nsms.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"dc912e7b-d1b9-7937-ebeb-5310b67017b1"},"source":"Now drop \"unnamed\" columns and rename v1 and v2 to \"label\" and \"message\""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"85bb6f9f-77db-6352-e53e-79bc29112304"},"outputs":[],"source":"sms = sms.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\nsms = sms.rename(columns = {'v1':'label','v2':'message'})"},{"cell_type":"markdown","metadata":{"_cell_guid":"e2fa50fd-b634-c505-c6a5-ec2155e32d2e"},"source":"Let's look into our data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0ad05287-4fa7-835a-2a58-ef273065875f"},"outputs":[],"source":"sms.groupby('label').describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"053b2b31-e65f-5514-5f07-a16b2c42f6c7"},"source":"Intresting that \"Sorry, I'll call later\" appears only 30 times here =)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d95f59db-4713-1f64-083c-fc8a25708234"},"source":"Now let's create new feature \"message length\" and plot it to see if it's of any interest"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6abbc5cb-3451-8de5-c9cc-0d4a04dbea02"},"outputs":[],"source":"sms['length'] = sms['message'].apply(len)\nsms.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4533b9ae-fea2-d346-321b-f73bd12d677d"},"outputs":[],"source":"mpl.rcParams['patch.force_edgecolor'] = True\nplt.style.use('seaborn-bright')\nsms.hist(column='length', by='label', bins=50,figsize=(11,5))"},{"cell_type":"markdown","metadata":{"_cell_guid":"0e5e7b86-5ad3-3a83-65a9-016bdb745904"},"source":"Looks like the lengthy is the message, more likely it is a spam. Let's not forget this"},{"cell_type":"markdown","metadata":{"_cell_guid":"2ae25190-c9b7-101b-6042-e8f9cf382e46"},"source":"### Text processing and vectorizing our meddages"},{"cell_type":"markdown","metadata":{"_cell_guid":"1e8ef576-3a86-7824-474d-d21e7dcba1f9"},"source":"Let's create new data frame. We'll need a copy later on"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1db101ba-5949-0c9c-6581-7b7d5e51de80"},"outputs":[],"source":"text_feat = sms['message'].copy()"},{"cell_type":"markdown","metadata":{"_cell_guid":"bd1d6d0c-39e2-65fb-1795-9c75bf51f29e"},"source":"Now define our tex precessing function. It will remove any punctuation and stopwords aswell."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"078389bf-00ec-13f4-78cd-f6ea818ff6f4"},"outputs":[],"source":"def text_process(text):\n    \n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n    \n    return \" \".join(text)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79b2e2b0-008d-b648-1031-f94c848a3064"},"outputs":[],"source":"text_feat = text_feat.apply(text_process)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6dce35d-b2b3-cfb5-42b1-a95aeac177a1"},"outputs":[],"source":"vectorizer = TfidfVectorizer(\"english\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9d11f55-f800-dd25-f001-dcc3c6a7fea4"},"outputs":[],"source":"features = vectorizer.fit_transform(text_feat)"},{"cell_type":"markdown","metadata":{"_cell_guid":"cfd14d8a-d700-ee85-f744-9d4562dd9fb0"},"source":"###  Classifiers and predictions"},{"cell_type":"markdown","metadata":{"_cell_guid":"08f8dcd4-2161-e373-2fc4-e9ecc631810d"},"source":"First of all let's split our features to test and train set"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"346766bb-d722-dca1-71d0-fd0f0dafcbae"},"outputs":[],"source":"features_train, features_test, labels_train, labels_test = train_test_split(features, sms['label'], test_size=0.3, random_state=111)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1eaef9ae-d68a-4001-8adb-0e2d031be876"},"source":"Now let's import bunch of classifiers, initialize them and make a dictionary to itereate through"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a4767634-1a31-ac41-bf62-34c66bfda0b1"},"outputs":[],"source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6e889ffa-59d8-377c-867d-25c67888765f"},"outputs":[],"source":"svc = SVC(kernel='sigmoid', gamma=1.0)\nknc = KNeighborsClassifier(n_neighbors=49)\nmnb = MultinomialNB(alpha=0.2)\ndtc = DecisionTreeClassifier(min_samples_split=7, random_state=111)\nlrc = LogisticRegression(solver='liblinear', penalty='l1')\nrfc = RandomForestClassifier(n_estimators=31, random_state=111)\nabc = AdaBoostClassifier(n_estimators=62, random_state=111)\nbc = BaggingClassifier(n_estimators=9, random_state=111)\netc = ExtraTreesClassifier(n_estimators=9, random_state=111)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f2211ff9-7718-6bc7-2231-d27fea07dc2d"},"source":"Parametres are based on notebook:\n[Spam detection Classifiers hyperparameter tuning][1]\n\n\n  [1]: https://www.kaggle.com/muzzzdy/d/uciml/sms-spam-collection-dataset/spam-detection-classifiers-hyperparameter-tuning/"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e9f8c83-33fb-312a-fb44-d6f8c955f427"},"outputs":[],"source":"clfs = {'SVC' : svc,'KN' : knc, 'NB': mnb, 'DT': dtc, 'LR': lrc, 'RF': rfc, 'AdaBoost': abc, 'BgC': bc, 'ETC': etc}"},{"cell_type":"markdown","metadata":{"_cell_guid":"030a45db-d84a-b8e2-30ca-09f3aff9501a"},"source":"Let's make functions to fit our classifiers and make predictions"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03099d8c-cac3-98a6-a68c-b6f22aed227c"},"outputs":[],"source":"def train_classifier(clf, feature_train, labels_train):    \n    clf.fit(feature_train, labels_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c74b2aba-431c-89e9-80be-0cf5ac718775"},"outputs":[],"source":"def predict_labels(clf, features):\n    return (clf.predict(features))"},{"cell_type":"markdown","metadata":{"_cell_guid":"5f0b2d9a-6301-3515-4363-ffcd6b8b6e1b"},"source":"Now iterate through classifiers and save the results"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"803e954a-ca19-091d-85d6-7f58f0df4879"},"outputs":[],"source":"pred_scores = []\nfor k,v in clfs.items():\n    train_classifier(v, features_train, labels_train)\n    pred = predict_labels(v,features_test)\n    pred_scores.append((k, [accuracy_score(labels_test,pred)]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52b248b9-679b-bb99-923a-97ce37440bfc"},"outputs":[],"source":"df = pd.DataFrame.from_items(pred_scores,orient='index', columns=['Score'])\ndf"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8f328a9-2fb9-70f3-97d7-77be91920131"},"outputs":[],"source":"df.plot(kind='bar', ylim=(0.9,1.0), figsize=(11,6), align='center', colormap=\"Accent\")\nplt.xticks(np.arange(9), df.index)\nplt.ylabel('Accuracy Score')\nplt.title('Distribution by Classifier')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9f0677da-7086-16bd-72ee-9b7773e450a6"},"source":"Looks like ensemble classifiers are not doing as good as expected."},{"cell_type":"markdown","metadata":{"_cell_guid":"bb9b55cc-0b1d-2542-0d55-48a5c4b83b81"},"source":"### Stemmer"},{"cell_type":"markdown","metadata":{"_cell_guid":"6ccb1920-3517-c118-7156-fad8f57c1691"},"source":"It is said that stemming short messages does no goot or even harm predictions. Let's try this out."},{"cell_type":"markdown","metadata":{"_cell_guid":"3f8663ef-bd68-2d8c-34b6-1231e7a665fe"},"source":"Define our stemmer function"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5b19298-c67e-bc13-58b7-0f3de5221189"},"outputs":[],"source":"def stemmer (text):\n    text = text.split()\n    words = \"\"\n    for i in text:\n            stemmer = SnowballStemmer(\"english\")\n            words += (stemmer.stem(i))+\" \"\n    return words"},{"cell_type":"markdown","metadata":{"_cell_guid":"6a5e7f03-abb3-f304-2732-614933b165de"},"source":"Stem, split, fit - repeat... Predict!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93da4610-5803-01b3-ff7c-17acc500650d"},"outputs":[],"source":"text_feat = text_feat.apply(stemmer)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d3e882d5-737d-13c6-53a3-99173de104a8"},"outputs":[],"source":"features = vectorizer.fit_transform(text_feat)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"433d51e7-f5bf-e9d6-1c67-02274f4f2504"},"outputs":[],"source":"features_train, features_test, labels_train, labels_test = train_test_split(features, sms['label'], test_size=0.3, random_state=111)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"67c866a0-cdc6-7b4a-45e9-97a9bc60bcb0"},"outputs":[],"source":"pred_scores = []\nfor k,v in clfs.items():\n    train_classifier(v, features_train, labels_train)\n    pred = predict_labels(v,features_test)\n    pred_scores.append((k, [accuracy_score(labels_test,pred)]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0fc7b0b4-6f5a-0729-bdfd-c2d963f72e0e"},"outputs":[],"source":"df2 = pd.DataFrame.from_items(pred_scores,orient='index', columns=['Score2'])\ndf = pd.concat([df,df2],axis=1)\ndf"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bdee0cfd-c0c8-c488-d763-2610bccff986"},"outputs":[],"source":"df.plot(kind='bar', ylim=(0.85,1.0), figsize=(11,6), align='center', colormap=\"Accent\")\nplt.xticks(np.arange(9), df.index)\nplt.ylabel('Accuracy Score')\nplt.title('Distribution by Classifier')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"},{"cell_type":"markdown","metadata":{"_cell_guid":"eb1afe61-9bf0-7a48-8553-913faa8f2ce3"},"source":"Looks like mostly the same . Ensemble classifiers doing a little bit better, NB still got the lead."},{"cell_type":"markdown","metadata":{"_cell_guid":"4359be28-c242-f619-d5e4-182a7db73716"},"source":"### What have we forgotten? Message length!"},{"cell_type":"markdown","metadata":{"_cell_guid":"90388ff7-bd5e-a224-3fab-e3d29242b5b1"},"source":"Let's append our message length feature to the matrix we fit into our classifiers"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"47579382-284c-0f3a-3292-d61a637a523e"},"outputs":[],"source":"lf = sms['length'].as_matrix()\nnewfeat = np.hstack((features.todense(),lf[:, None]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dc824a5a-ca6f-423a-89cd-512cfe07e4cb"},"outputs":[],"source":"features_train, features_test, labels_train, labels_test = train_test_split(newfeat, sms['label'], test_size=0.3, random_state=111)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5470c596-720f-3d1f-cfeb-02cfcc4a7bac"},"outputs":[],"source":"pred_scores = []\nfor k,v in clfs.items():\n    train_classifier(v, features_train, labels_train)\n    pred = predict_labels(v,features_test)\n    pred_scores.append((k, [accuracy_score(labels_test,pred)]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7a77c087-88ed-f1c8-e2f4-fe135886af66"},"outputs":[],"source":"df3 = pd.DataFrame.from_items(pred_scores,orient='index', columns=['Score3'])\ndf = pd.concat([df,df3],axis=1)\ndf"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e84b01e2-b9ed-6d4f-e667-5183ef30ad7b"},"outputs":[],"source":"df.plot(kind='bar', ylim=(0.85,1.0), figsize=(11,6), align='center', colormap=\"Accent\")\nplt.xticks(np.arange(9), df.index)\nplt.ylabel('Accuracy Score')\nplt.title('Distribution by Classifier')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"},{"cell_type":"markdown","metadata":{"_cell_guid":"101bda42-19fe-ff14-7a30-e7bad174207c"},"source":"This time everyone are doing a little bit worse, except for LinearRegression and RandomForest. But the winner is still MultinominalNaiveBayes."},{"cell_type":"markdown","metadata":{"_cell_guid":"cbf377a8-4f68-b82a-3c19-e26f73f40e35"},"source":"### Voting classifier"},{"cell_type":"markdown","metadata":{"_cell_guid":"7e8131cd-5934-4411-9687-f61a27751a95"},"source":"We are using ensemble algorithms here, but what about ensemble of ensembles? Will it beat NB?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e76bd4e6-6384-dcc7-2d5e-1731383c3af7"},"outputs":[],"source":"from sklearn.ensemble import VotingClassifier"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d59abce-ba88-1f77-b6cd-2f992fec95ac"},"outputs":[],"source":"eclf = VotingClassifier(estimators=[('BgC', bc), ('ETC', etc), ('RF', rfc), ('Ada', abc)], voting='soft')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"743afa06-5134-466b-cc31-18f54eb93f0b"},"outputs":[],"source":"eclf.fit(features_train,labels_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d834da3c-3dc9-e209-0c80-9f93c3d8a519"},"outputs":[],"source":"pred = eclf.predict(features_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4de3ce6c-3c95-9fe5-e5dd-0888db0bbb12"},"outputs":[],"source":"print(accuracy_score(labels_test,pred))"},{"cell_type":"markdown","metadata":{"_cell_guid":"7f834a34-00d0-d310-91fd-4e5282811ae4"},"source":"Better but nope."},{"cell_type":"markdown","metadata":{"_cell_guid":"0437b30e-96a9-2ef5-44f8-7dc6b94ed8a6"},"source":"### Final verdict - well tuned NaiveBayes is your friend in spam detection."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d18c773a-c280-fa11-cd9c-37fd5046b78a"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}